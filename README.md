# Awesome Unified Multimodal Understanding and Generation

A curated list of papers, models, and resources for **unified multimodal understanding and generation**, covering foundational works, auto-regressive models, diffusion models, and state-of-the-art unified frameworks. This repository aims to provide researchers and practitioners with a comprehensive resource to explore advancements in this field.

## Categories
- **Multimodal Understanding**: Explore models designed for comprehending multimodal data, including image-text alignment, visual reasoning, and contextual understanding.
- **Multimodal Generation**: Discover approaches for generating multimodal content, including images, text, and other modalities.
- **Unified Frameworks**: Investigate works that unify multimodal understanding and generation using innovative techniques like autoregressive modeling and diffusion models.

---

## Previous Works on Multimodal Understanding and Generation

### Multimodal Understanding
- **LLaVA**
- **MiniGPT4**
- **BLIP**
- **Deepseek-VL**
- **InternVL**
- **Qwen-VL**

### Multimodal Generation

#### Auto-regressive Models
- **PixelRNN**
- **PixelCNN**
- **LLamagen**

#### Diffusion Models
##### Continuous Diffusion
- **DDPM**

##### Discrete Diffusion
- **D3PM**
- **MaskGIT**

---

## Unified Multimodal Understanding and Generation

These works unify multimodal understanding and generation by integrating autoregressive methods, diffusion models, and other innovative techniques.

- **DreamLLM**  
  *Runpei Dong et al.*  
  *"Synergistic multimodal comprehension and creation."*  
  [arXiv:2309.11499](https://arxiv.org/abs/2309.11499) (2023)

- **Chameleon**  
  *Chameleon Team*  
  *"Mixed-modal early-fusion foundation models."*  
  [arXiv:2405.09818](https://arxiv.org/abs/2405.09818) (2024)

- **TransFusion**  
  *Chunting Zhou et al.*  
  *"Predict the next token and diffuse images with one multi-modal model."*  
  [arXiv:2408.11039](https://arxiv.org/abs/2408.11039) (2024)

- **Next-GPT**  
  *Shengqiong Wu et al.*  
  *"Any-to-any multimodal LLM."*  
  [arXiv:2309.05519](https://arxiv.org/abs/2309.05519) (2023)

- **Janus**  
  *Chengyue Wu et al.*  
  *"Decoupling visual encoding for unified multimodal understanding and generation."*  
  [arXiv:2410.13848](https://arxiv.org/abs/2410.13848) (2024)

- **JanusFlow**  
  *Yiyang Ma et al.*  
  *"Harmonizing autoregression and rectified flow for unified multimodal understanding and generation."*  
  [arXiv:2411.07975](https://arxiv.org/abs/2411.07975) (2024)

- **Emu3**  
  *Xinlong Wang et al.*  
  *"Next-token prediction is all you need."*  
  [arXiv:2409.18869](https://arxiv.org/abs/2409.18869) (2024)

- **Show-O**  
  *Jinheng Xie et al.*  
  *"One single transformer to unify multimodal understanding and generation."*  
  [arXiv:2408.12528](https://arxiv.org/abs/2408.12528) (2024)

- **TokenFlow**  
  *Liao Qu et al.*  
  *"Unified image tokenizer for multimodal understanding and generation."*  
  [arXiv:2412.03069](https://arxiv.org/abs/2412.03069) (2024)

---

## Contributions
Feel free to contribute by adding missing papers, models, or resources! Submit a pull request or open an issue with relevant details.

---

## License
This repository is licensed under the [MIT License](LICENSE).

---

Happy exploring!
